# -*- coding: utf-8 -*-
"""Detection of Suicidal Ideation from their Tweets using NLP & DL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V_g0NkNuOqEkH8oVRpUb9VWYtwL4li1T

# **Importing Necessary Libraries**
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report

"""# **Exploring the Dataset**"""

from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/MyDrive/Suicidal Tweet Dataset.zip' -d '/content/drive/MyDrive/Suicidal Tweet Dataset_unzip'

# Sample dataset containing tweets labeled as suicidal or non-suicidal
tweets_data = pd.read_csv('/content/drive/MyDrive/Suicidal Tweet Dataset_unzip/Suicide_Ideation_Dataset(Twitter-based).csv')

tweets_data.head(15)

import random
index = random.randrange(0, tweets_data.shape[0])
print("Tweet is:\n", tweets_data.iloc[index]['Tweet'])
print("Sentiment is:\n", tweets_data.iloc[index]['Suicide'])

"""# **Total Tweets in Dataset**"""

num_tweets = tweets_data.shape[0]
print("Total number of tweets in the dataset: ", num_tweets)

"""# **Total Tweets of each Class**"""

not_suicide = tweets_data[tweets_data['Suicide']=='Not Suicide post']['Suicide'].count()

potential_suicide = tweets_data[tweets_data['Suicide']=='Potential Suicide post ']['Suicide'].count()

print("Number of not suicide tweets: ", not_suicide)
print("Number of potential suicide tweets: ", potential_suicide)

"""# **Total Ratio of State of Tweets: Potential Suicide vs Not Suicide**

(1) **BAR GRAPH**
"""

x = ['not suicide post', 'potential suicide post', ]
y = [not_suicide, potential_suicide]

plt.bar(x, y, color=['r', 'y'])
plt.title('Bar graph of tweets class')
plt.show()

"""(2) **PIE CHART**"""

plt.pie(y, labels=x, colors=['y', 'g'])
plt.title("Pie chart of tweets class")
plt.show()

"""# **One-Hot Encoding**"""

tweets_data['Suicide'] = tweets_data['Suicide'].replace({'Not Suicide post': 0, 'Potential Suicide post ': 1})

tweets_data.head(15)

print("Not Suicide post: ", tweets_data[tweets_data['Suicide']==0]['Suicide'].count())

print("Potential Suicide post:", tweets_data[tweets_data['Suicide']==1]['Suicide'].count())

"""# **Pre-processing the Text**"""

# Preprocessing the text
#Conversion of Tweet column of dataframe to list
tweets = tweets_data['Tweet'].astype('str')
tweets = tweets.tolist()
print("Type is: ", type(tweets))

# Conversion of Suicide column of dataframe to list
labels = tweets_data['Suicide'].tolist()
print("Type is: ", type(labels))
# Conversion of Suicide column to array
labels = np.array(labels)

# Random tweets from the list
rand_index = random.randrange(0, len(tweets))
print("Tweet is: \n", tweets[rand_index])
if labels[rand_index]==0:
    print("Not Suicide(0)")
else:
    print("Potential Suicide(1)")

"""# **Splitting of Dataset into Train & Test**"""

# Splitting data into training and testing sets
train_tweet, test_tweet, train_label, test_label = train_test_split(tweets, labels, random_state=42, test_size=0.2)

train_tweet[2]

train_label[2]

"""# ***Pre-processing the Textual Data into Numerical Format ***

**I. Setting up the hyperparameters**
"""

words_count = []
for i in tweets:
    words_count.append(len(i.split()))

max_length = max(words_count) # maximum number of words in a sentence, max length our sequence will be
vocab_size = 1000 # maximum number of unique words in the dataset
pad_type = 'post'
trunc_type = 'pre'
oov_tok = "<OOV>"
embedding_dim = round(np.sqrt(vocab_size))

"""**II. Tokenization & Padded Sequence on Train & Test Data**"""

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_tweet)
word_indexes = tokenizer.word_index

"""**(a) Train Data**"""

training_sequences = tokenizer.texts_to_sequences(train_tweet)
training_padded = pad_sequences(training_sequences, maxlen=max_length,
                               padding=pad_type, truncating=trunc_type)

"""**(b) Test Data**"""

testing_sequences = tokenizer.texts_to_sequences(test_tweet)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length,
                              padding=pad_type, truncating=trunc_type)

"""# **Applying Deep Learning Model**"""

# Building the neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(max_words, 16, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compilation of a Model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# Training the model
epochs = 30
history = model.fit(training_padded, train_label, epochs=epochs, validation_data=(testing_padded, test_label), verbose=2)

# Evaluate the model
loss, accuracy = model.evaluate(testing_padded, test_label)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""# **Representation of Accuracy & Loss Curve of DL in Graph**

**(a) Accuracy Curve**
"""

def accuracy_plot(model):
    accuracy = model.history['accuracy']
    val_accuracy = model.history['val_accuracy']
    epochs = range(len(history.history['accuracy']))

    plt.plot(epochs, accuracy, label='training_accuracy')
    plt.plot(epochs, val_accuracy, label='val_accuracy')
    plt.title('accuracy')
    plt.xlabel('epochs')
    plt.legend()

accuracy_plot(history)

"""**(b) Loss Curve**"""

def loss_plot(model):
    loss = model.history['loss']
    val_loss = model.history['val_loss']
    epochs = range(len(history.history['loss']))

    plt.plot(epochs, loss, label='training_loss')
    plt.plot(epochs, val_loss, label='val_loss')
    plt.title('loss')
    plt.xlabel('epochs')
    plt.legend()

loss_plot(history)

"""# **Model Prediction on Test Data**"""

import time
import sys
def load(count=50, filler="=", delay=0.02):
  for i in range(count + 1):
    sys.stdout.write('\r')
    sys.stdout.write("[%s%s]" % (filler * i, ' ' * (count - i)))
    sys.stdout.flush()
    time.sleep(delay)

def userdef_tweet():
    custom_tweet = input("Enter a tweet: ")
    print()
    print("Your tweet is: ", custom_tweet)
    sentence = []
    sentence.append(custom_tweet)
    test_s = tokenizer.texts_to_sequences(sentence)
    test_p = pad_sequences(test_s, maxlen=max_length,
                      padding=pad_type, truncating=trunc_type)
    pred_value = model.predict(test_p)
    print("Predict Value is: ", pred_value)
    load(filler=">")
    print()
    if(pred_value>0.5):
        print("Potential Suicide Tweet")
    else:
        print("Not a suicide Tweet")

userdef_tweet()

userdef_tweet()

userdef_tweet()

userdef_tweet()

userdef_tweet()